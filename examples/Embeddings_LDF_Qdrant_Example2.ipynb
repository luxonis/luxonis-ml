{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import onnxruntime\n",
    "from utils.torch_utils import (\n",
    "    load_model_resnet50, \n",
    "    load_model_resnet50_minuslastlayer, \n",
    "    export_model_onnx,\n",
    "    extract_embeddings_onnx,\n",
    "    extract_embeddings_torch,\n",
    "    save_embeddings,\n",
    "    load_embeddings,\n",
    ")\n",
    "from utils.data_utils import load_mnist_data\n",
    "from luxonis_ml.embeddings.utils.model import (\n",
    "    load_model_onnx, \n",
    "    save_model_onnx, \n",
    "    extend_output_onnx,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### PyTorch Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_loader = load_mnist_data(save_path=\"./data/mnist\", num_samples=640, batch_size=64)\n",
    "\n",
    "# Load the PyTorch model\n",
    "model = load_model_resnet50_minuslastlayer()\n",
    "\n",
    "# Extract and save embeddings\n",
    "embeddings, labels = extract_embeddings_torch(model, data_loader)\n",
    "save_embeddings(embeddings, labels, \"./data/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ONNX Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the ONNX model\n",
    "model = load_model_resnet50()\n",
    "export_model_onnx(model, model_path_out=\"./data/resnet50.onnx\")\n",
    "onnx_model = load_model_onnx(model_path=\"./data/resnet50.onnx\")\n",
    "onnx_model = extend_output_onnx(onnx_model, intermediate_tensor_name=\"/Flatten_output_0\")\n",
    "save_model_onnx(onnx_model, model_path_out=\"./data/resnet50-1.onnx\")\n",
    "\n",
    "# Create an ONNX Runtime session\n",
    "provider = [\"CUDAExecutionProvider\"] if torch.cuda.is_available() and \"CUDAExecutionProvider\" in onnxruntime.get_available_providers() else None\n",
    "ort_session = onnxruntime.InferenceSession(\"./data/resnet50-1.onnx\", providers=provider)\n",
    "\n",
    "# Extract and save ONNX embeddings\n",
    "embeddings, labels = extract_embeddings_onnx(ort_session, data_loader, \"/Flatten_output_0\")\n",
    "save_embeddings(embeddings, labels, \"./data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From LDF (Building LDF and Generating Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from luxonis_ml.data import LuxonisDataset, LuxonisLoader\n",
    "from luxonis_ml.embeddings.utils.ldf import generate_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "mnist_image_dir = \"./data/mnist_images\"\n",
    "\n",
    "# Convert MNIST data to Luxonis ML format\n",
    "def mnist_LDF_generator():\n",
    "    batch_num = 0\n",
    "    for batch in data_loader:\n",
    "        images, labels = batch\n",
    "        for i, (image, label) in enumerate(zip(images, labels)):\n",
    "            img_ix = batch_num * BATCH_SIZE + i\n",
    "\n",
    "            # Save image to disk\n",
    "            # image_path = os.path.join(mnist_image_dir, f\"{uuid.uuid4()}.jpg\")\n",
    "            image_path = os.path.join(mnist_image_dir, f\"mnist_{img_ix}.jpg\")\n",
    "            torchvision.utils.save_image(image, image_path)\n",
    "\n",
    "            # Create dictionary structure for Luxonis ML\n",
    "            yield {\n",
    "                \"file\": image_path,\n",
    "                \"class\": str(label.item()),\n",
    "                \"type\": \"classification\",\n",
    "                \"value\": True,\n",
    "            }\n",
    "        batch_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the MNIST data is already loaded and processed into images\n",
    "# Convert MNIST data to LDF and generate embeddings\n",
    "dataset_name = \"Mnist_LDF\"\n",
    "dataset = LuxonisDataset(dataset_name)\n",
    "dataset.set_classes([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"])\n",
    "dataset.add(mnist_LDF_generator)  # Assuming mnist_LDF_generator is defined\n",
    "dataset.make_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from luxonis_ml.embeddings.utils.qdrant import QdrantAPI, QdrantManager\n",
    "QdrantManager(\"qdrant/qdrant\", \"qdrant_container\").start_docker_qdrant()\n",
    "qdrant_api = QdrantAPI(\"localhost\", 6333)\n",
    "# Create a collection\n",
    "vector_size = len(embeddings[0])\n",
    "qdrant_api.create_collection(\n",
    "    collection_name=\"Mnist\", \n",
    "    properties=[\"label\",\"image_path\"], \n",
    "    vector_size=vector_size, \n",
    "    distance=\"cosine\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = LuxonisLoader(dataset)\n",
    "ort_session = onnxruntime.InferenceSession(\"./data/resnet50-1.onnx\", providers=provider)\n",
    "emb_dict = generate_embeddings(dataset, ort_session, qdrant_api, output_layer_name=\"/Flatten_output_0\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Without LDF (Using LuxonisFileSystem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from luxonis_ml.embeddings.utils.embedding import extract_embeddings\n",
    "from luxonis_ml.utils import LuxonisFileSystem\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "file_info = {\n",
    "    \"base_path\": current_dir,\n",
    "    \"prefix\": \"data/mnist_images\",\n",
    "    \"model_path\": \"data/resnet50-1.onnx\",\n",
    "}\n",
    "local_model = file_info[\"model_path\"]\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(local_model, providers=provider)\n",
    "lfs_local = LuxonisFileSystem(file_info[\"base_path\"])\n",
    "\n",
    "files = []\n",
    "for img in lfs_local.walk_dir(file_info[\"prefix\"]):\n",
    "    files.append(img)\n",
    "\n",
    "embeddings, succ_ix = extract_embeddings(files, ort_session, lfs_local, preprocess_function=None, output_layer_name=\"/Flatten_output_0\", batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from luxonis_ml.embeddings.utils.qdrant import QdrantAPI, QdrantManager\n",
    "\n",
    "QdrantManager(\"qdrant/qdrant\", \"qdrant_container\").start_docker_qdrant()\n",
    "qdrant_api = QdrantAPI(\"localhost\", 6333)\n",
    "\n",
    "# Create a collection\n",
    "vector_size = len(embeddings[0])\n",
    "qdrant_api.create_collection(\n",
    "    collection_name=\"Mnist_LFS\", \n",
    "    properties=[\"image_path\"], \n",
    "    vector_size=vector_size, \n",
    "    distance=\"cosine\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "# Insert the embeddings into the collection\n",
    "ids = [str(uuid.uuid5(uuid.NAMESPACE_DNS, str(e))) for e in embeddings]\n",
    "img_path_list_dict = [{\"image_path\": img} for i, img in enumerate(files) if i in succ_ix]\n",
    "qdrant_api.insert_embeddings(ids, embeddings, img_path_list_dict, batch_size=50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
