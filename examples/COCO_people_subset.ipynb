{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9de6101",
   "metadata": {},
   "source": [
    "## Adding a subset of COCO people data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c06d8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, json, os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pycocotools.mask as mask_util\n",
    "from PIL import Image, ImageDraw\n",
    "from copy import deepcopy\n",
    "\n",
    "from luxonis_ml.data import *\n",
    "from luxonis_ml.loader import *\n",
    "\n",
    "BUCKET_TYPE = 'local'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718c2791",
   "metadata": {},
   "source": [
    "### Download and extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cc9ddf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /home/conor/Luxonis/envs/fiftyone/lib/python3.9/site-packages (4.7.1)\n",
      "Requirement already satisfied: filelock in /home/conor/Luxonis/envs/fiftyone/lib/python3.9/site-packages (from gdown) (3.12.2)\n",
      "Requirement already satisfied: six in /home/conor/Luxonis/envs/fiftyone/lib/python3.9/site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /home/conor/Luxonis/envs/fiftyone/lib/python3.9/site-packages (from gdown) (4.65.0)\n",
      "Requirement already satisfied: requests[socks] in /home/conor/Luxonis/envs/fiftyone/lib/python3.9/site-packages (from gdown) (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/conor/Luxonis/envs/fiftyone/lib/python3.9/site-packages (from gdown) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/conor/Luxonis/envs/fiftyone/lib/python3.9/site-packages (from beautifulsoup4->gdown) (2.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/conor/Luxonis/envs/fiftyone/lib/python3.9/site-packages (from requests[socks]->gdown) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/conor/Luxonis/envs/fiftyone/lib/python3.9/site-packages (from requests[socks]->gdown) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/conor/Luxonis/envs/fiftyone/lib/python3.9/site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/conor/Luxonis/envs/fiftyone/lib/python3.9/site-packages (from requests[socks]->gdown) (3.1.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/conor/Luxonis/envs/fiftyone/lib/python3.9/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1XlvFK7aRmt8op6-hHkWVKIJQeDtOwoRT\n",
      "To: /home/conor/Luxonis/luxonis-ml/data/COCO_people_subset.zip\n",
      "100%|██████████████████████████████████████| 7.78M/7.78M [00:00<00:00, 17.7MB/s]\n",
      "Archive:  ../data/COCO_people_subset.zip\n",
      "replace ../data/person_keypoints_val2017.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "! pip install gdown\n",
    "! gdown 1XlvFK7aRmt8op6-hHkWVKIJQeDtOwoRT -O ../data/COCO_people_subset.zip\n",
    "! unzip ../data/COCO_people_subset.zip -d ../data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94150fc8",
   "metadata": {},
   "source": [
    "### Find image paths and load COCO annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fa5d2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = '../data/person_val2017_subset'\n",
    "annot_file = '../data/person_keypoints_val2017.json'\n",
    "\n",
    "# get paths to images sorted by number\n",
    "im_paths = glob.glob(img_dir+'/*.jpg')\n",
    "nums = np.array([int(path.split('/')[-1].split('.')[0]) for path in im_paths])\n",
    "idxs = np.argsort(nums)\n",
    "im_paths = list(np.array(im_paths)[idxs])\n",
    "\n",
    "# load \n",
    "with open(annot_file) as file:\n",
    "    data = json.load(file)\n",
    "imgs = data['images']\n",
    "anns = data['annotations']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2befa6b3",
   "metadata": {},
   "source": [
    "### Convert from COCO format\n",
    "\n",
    "The LDF will expect the following format:\n",
    "```\n",
    "additions (list[dict]): {\n",
    "    # path to the image on local storage\n",
    "    'filepath' (str): \"my_image.jpg\",\n",
    "    # list of boxes, which are length 6 lists of [class, x, y, width, height]\n",
    "    'boxes' (list[list]): [[\"apple\", 0.1, 0.2, 0.1, 0.2], ...],\n",
    "    # list of keypoint instances, which are length 2 [class, points] and points is a list of (x,y) tuples\n",
    "    'keypoints' (list[str|int, list[tuple]]): [[\"apple\", [(0.1, 0.2), (0.3, 0.4), ...], ...],\n",
    "    # an integer numpy array of HxW where the values correspond to classes\n",
    "    'segmentation' (np.ndarray): np.zeros((height, width))\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4404049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some artificial splits\n",
    "splits = ['train' for _ in range(20)] + ['val' for _ in range(10)]\n",
    "\n",
    "additions = [] # list of additional training examples\n",
    "\n",
    "for i, path in enumerate(im_paths):\n",
    "    # find annotations matching the COCO image\n",
    "    gran = path.split('/')[-1]\n",
    "    img = [img for img in imgs if img['file_name']==gran][0]\n",
    "    img_id = img['id']\n",
    "    img_anns = [ann for ann in anns if ann['image_id'] == img_id]\n",
    "    \n",
    "    # load the image\n",
    "    im = cv2.imread(path)\n",
    "    height, width, _ = im.shape\n",
    "    \n",
    "    # initialize annotations for LDF\n",
    "    mask = np.zeros((height, width)) # segmentation mask is always a HxW numpy array\n",
    "    boxes = [] # bounding boxes are a list of [class, x, y, width, height] of the box\n",
    "    keypoints = [] # keypoints are a list of classes and (x,y) points\n",
    "    \n",
    "    for ann in img_anns:\n",
    "        # COCO-specific conversion for segmentation\n",
    "        seg = ann['segmentation']\n",
    "        if isinstance(seg, list):\n",
    "            for s in seg:\n",
    "                poly = np.array(s).reshape(-1,2)\n",
    "                poly = [(poly[i,0],poly[i,1]) for i in range(len(poly))]\n",
    "                m = Image.new('L', (width, height), 0)\n",
    "                ImageDraw.Draw(m).polygon(poly, outline=1, fill=1)\n",
    "                m = np.array(m)\n",
    "                mask[m==1] = 1\n",
    "        \n",
    "        # COCO-specific conversion for bounding boxes\n",
    "        x, y, w, h = ann['bbox']\n",
    "        boxes.append(['person', x/width, y/height, w/width, h/height])\n",
    "        \n",
    "        # COCO-specific conversion for keypoints\n",
    "        kps = np.array(ann['keypoints']).reshape(-1, 3)\n",
    "        keypoint = []\n",
    "        for kp in kps:\n",
    "            if kp[2] == 0:\n",
    "                keypoint.append((float('nan'), float('nan')))\n",
    "            else: \n",
    "                keypoint.append((kp[0]/width, kp[1]/height))  \n",
    "        keypoints.append(['person', keypoint])\n",
    "    \n",
    "    # dictionary structure expected by LDF\n",
    "    additions.append({\n",
    "        'image': {\n",
    "            'filepath': path,\n",
    "            'segmentation': mask,\n",
    "            'boxes': boxes,\n",
    "            'keypoints': keypoints,\n",
    "            'split': splits[i]\n",
    "        }\n",
    "    })\n",
    "    \n",
    "original_additions = deepcopy(additions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8171a7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset id: 64ad8195fb15ff76db990fcb\n"
     ]
    }
   ],
   "source": [
    "team_id = \"luxonis_team_id\"\n",
    "team_name = \"luxonis\"\n",
    "dataset_name = \"coco_test\"\n",
    "\n",
    "dataset_id = LuxonisDataset.create(\n",
    "    team_id=team_id,\n",
    "    team_name=team_name,\n",
    "    dataset_name=dataset_name\n",
    "\n",
    ")\n",
    "print(f\"Dataset id: {dataset_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a30216e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 141381.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for additions or modifications...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:01<00:00, 18.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset media...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 127.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████████| 30/30 [2.4s elapsed, 0s remaining, 12.7 samples/s]   \n"
     ]
    }
   ],
   "source": [
    "with LuxonisDataset(\n",
    "    team_id=team_id,\n",
    "    dataset_id=dataset_id,\n",
    "    team_name=team_name,\n",
    "    dataset_name=dataset_name,\n",
    "    bucket_type=BUCKET_TYPE\n",
    ") as dataset:\n",
    "    \n",
    "    # defines the format of the COCO data source,\n",
    "    # which in this case is just a single image per training example\n",
    "    dataset.create_source(\n",
    "        'coco',\n",
    "        custom_components=[\n",
    "            LDFComponent('image', HType.IMAGE, IType.BGR)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    dataset.set_classes(['person']) # needed for classification and detection\n",
    "    dataset.set_mask_targets({1:'person'}) # needed for segmentation\n",
    "    dataset.set_skeleton({\n",
    "        'labels': data['categories'][0]['keypoints'],\n",
    "        'edges': (np.array(data['categories'][0]['skeleton'])-1).tolist()\n",
    "    }) # optional for keypoints to define the skeleton visualization\n",
    "    \n",
    "    dataset.add(additions)\n",
    "    dataset.create_version(note=\"Initially adding data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58991ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 186137.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for additions or modifications...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:04<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset media...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 30320.27it/s]\n"
     ]
    }
   ],
   "source": [
    "with LuxonisDataset(\n",
    "    team_id=team_id,\n",
    "    dataset_id=dataset_id,\n",
    "    bucket_type=BUCKET_TYPE\n",
    ") as dataset:\n",
    "    \n",
    "    additions = deepcopy(original_additions)\n",
    "    \n",
    "    # add a classification label to the dataset\n",
    "    for addition in additions:\n",
    "        addition['image']['class'] = 'person'\n",
    "    \n",
    "    dataset.add(additions)\n",
    "    dataset.create_version(note=\"Add classification labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e60d5a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset version: 1.1\n",
      "Class attribute presence in version 1.0:\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Class attribute presence in version 1.1:\n",
      "[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "with LuxonisDataset(\n",
    "    team_id=team_id,\n",
    "    dataset_id=dataset_id,\n",
    ") as dataset:\n",
    "    \n",
    "    print(\"Dataset version:\", dataset.version)\n",
    "    v10 = dataset.fo_dataset.load_saved_view('version_1.0')\n",
    "    print(\"Class attribute presence in version 1.0:\")\n",
    "    print([sample['class'] is not None for sample in v10])\n",
    "    v11 = dataset.fo_dataset.load_saved_view('version_1.1')\n",
    "    print(\"Class attribute presence in version 1.1:\")\n",
    "    print([sample['class'] is not None for sample in v11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d160b62a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'BaseDict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m LuxonisDataset(\n\u001b[1;32m      2\u001b[0m     team_id\u001b[38;5;241m=\u001b[39mteam_id,\n\u001b[1;32m      3\u001b[0m     dataset_id\u001b[38;5;241m=\u001b[39mdataset_id,\n\u001b[1;32m      4\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m dataset:\n\u001b[1;32m      6\u001b[0m     loader \u001b[38;5;241m=\u001b[39m LuxonisLoader(dataset)\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(data))\n",
      "File \u001b[0;32m~/Luxonis/luxonis-ml/src/luxonis_ml/loader/loader.py:98\u001b[0m, in \u001b[0;36mLuxonisLoader.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m kps \u001b[38;5;129;01min\u001b[39;00m sample\u001b[38;5;241m.\u001b[39mkeypoints\u001b[38;5;241m.\u001b[39mkeypoints:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses\u001b[38;5;241m.\u001b[39mindex(kps\u001b[38;5;241m.\u001b[39mlabel)\n\u001b[0;32m---> 98\u001b[0m     pnts \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoints\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     kps \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(pnts), \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m    100\u001b[0m     nan_key \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39misnan(pnts[:, \u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'BaseDict'"
     ]
    }
   ],
   "source": [
    "with LuxonisDataset(\n",
    "    team_id=team_id,\n",
    "    dataset_id=dataset_id,\n",
    ") as dataset:\n",
    "    \n",
    "    loader = LuxonisLoader(dataset)\n",
    "    for data in loader:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcc8595",
   "metadata": {},
   "source": [
    "### Permanently delete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2724865d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'_dataset_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m LuxonisDataset(\n\u001b[1;32m      2\u001b[0m     team_id\u001b[38;5;241m=\u001b[39mteam_id,\n\u001b[1;32m      3\u001b[0m     dataset_id\u001b[38;5;241m=\u001b[39mdataset_id,\n\u001b[1;32m      4\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m dataset:\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelete_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Luxonis/luxonis-ml/src/luxonis_ml/data/dataset.py:469\u001b[0m, in \u001b[0;36mLuxonisDataset.delete_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    463\u001b[0m ldf_doc \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconn\u001b[38;5;241m.\u001b[39mluxonis_source_document\u001b[38;5;241m.\u001b[39mdelete_many(\n\u001b[1;32m    466\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_luxonis_dataset_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: ldf_doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m    467\u001b[0m )\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconn\u001b[38;5;241m.\u001b[39mversion_document\u001b[38;5;241m.\u001b[39mdelete_many(\n\u001b[0;32m--> 469\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_dataset_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mldf_doc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_dataset_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m}\n\u001b[1;32m    470\u001b[0m )\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconn\u001b[38;5;241m.\u001b[39mtransaction_document\u001b[38;5;241m.\u001b[39mdelete_many({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_dataset_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: ldf_doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconn\u001b[38;5;241m.\u001b[39mluxonis_dataset_document\u001b[38;5;241m.\u001b[39mdelete_many(\n\u001b[1;32m    473\u001b[0m     {\n\u001b[1;32m    474\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$and\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    478\u001b[0m     }\n\u001b[1;32m    479\u001b[0m )\n",
      "\u001b[0;31mKeyError\u001b[0m: '_dataset_id'"
     ]
    }
   ],
   "source": [
    "with LuxonisDataset(\n",
    "    team_id=team_id,\n",
    "    dataset_id=dataset_id,\n",
    ") as dataset:\n",
    "    \n",
    "    dataset.delete_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
